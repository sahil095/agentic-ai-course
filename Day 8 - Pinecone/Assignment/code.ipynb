{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc6131d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from langchain import hub\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_core.prompts import PromptTemplate\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "# from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82f2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ['PINECONE_API_KEY']=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64732170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 247 page documents (should cover >200 pages).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load all PDFs from directory\n",
    "pdf_dir = \"./data/\"\n",
    "pdf_files = glob.glob(os.path.join(pdf_dir, \"*.pdf\"))\n",
    "\n",
    "all_docs = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    all_docs.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} page documents (should cover >200 pages).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c678d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total semantic chunks: 318\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Semantic Chunking\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "chunker = SemanticChunker(embedding_model, breakpoint_threshold_type=\"standard_deviation\", min_chunk_size=100)\n",
    "semantic_chunks = chunker.create_documents([doc.page_content for doc in all_docs])\n",
    "\n",
    "print(f\"Total semantic chunks: {len(semantic_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d3fb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [chunk.page_content for chunk in semantic_chunks]\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e28f4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc=Pinecone()\n",
    "\n",
    "index_name=\"agenticbatch2-assignment\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")    \n",
    ")\n",
    "    \n",
    "#loading the index\n",
    "index=pc.Index(index_name)\n",
    "vector_store=PineconeVectorStore(index=index,embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc054e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX_DIM = 768  # Must match your embedding size\n",
    "\n",
    "# # Flat index\n",
    "# pc.create_index(\n",
    "#     name=\"rag-flat\",\n",
    "#     dimension=INDEX_DIM,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"gcp\", region=\"us-east1\"),\n",
    "#     pod_type=\"s1\",     # or appropriate for your plan\n",
    "#     index_type=\"POD\",  # POD is Pinecone's Flat index\n",
    "# )\n",
    "\n",
    "# # HNSW index\n",
    "# pc.create_index(\n",
    "#     name=\"rag-hnsw\",\n",
    "#     dimension=INDEX_DIM,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"gcp\", region=\"us-east1\"),\n",
    "#     pod_type=\"s1\",\n",
    "#     index_type=\"HNSW\"\n",
    "# )\n",
    "\n",
    "# # IVF_PQ index (Pinecone's version of IVF)\n",
    "# pc.create_index(\n",
    "#     name=\"rag-ivfpq\",\n",
    "#     dimension=INDEX_DIM,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"gcp\", region=\"us-east1\"),\n",
    "#     pod_type=\"s1\",\n",
    "#     index_type=\"IVF_PQ\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779ff493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each item: (id, vector, metadata)\n",
    "items = [\n",
    "    (str(uuid4()), emb, {\"text\": text}) for emb, text in zip(embeddings, texts)\n",
    "]\n",
    "\n",
    "batch_size = 10\n",
    "for i in range(0, len(items), batch_size):\n",
    "    index.upsert(vectors=items[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "648a6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=10, threshold=0.7):\n",
    "    query_emb = embedding_model.embed_query(query)\n",
    "    # Query Pinecone\n",
    "    result = index.query(\n",
    "        vector=query_emb,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter=None,  # Optionally filter on metadata\n",
    "        include_values=True\n",
    "    )\n",
    "    # Filter by score threshold\n",
    "    matches = [m for m in result['matches'] if m['score'] >= threshold]\n",
    "    return matches, query_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47c58b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time: 0.3317s\n",
      "Score: 0.80, Text: 21, 2022. 2, 10, 14,\n",
      "16, 18, 19\n",
      "[31] Z. W ang et al. , “ A Medical Semantic-Assisted Transformer for Radio-\n",
      "graphic Report Generation. ”, Med. Image Comput. Assist Interv ., V ol\n",
      "13433, pp. 655–664 , ...\n",
      "Score: 0.79, Text: 22 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. XX, NO. XX, X XXX 2023\n",
      "[33] C. Shang et al., “MA TNet: Exploiting Multi-Modal Features for Radiol-\n",
      "ogy Report Generation, ” IEEE Signal Process. Lett. ,...\n",
      "Score: 0.78, Text: Informatics in Medicine Unlocked 24 (2021) 100557\n",
      "Available online 26 March 2021\n",
      "2352-9148/© 2021 The Authors. Published by Elsevier Ltd....\n",
      "Score: 0.78, Text: . 8\n",
      "2.2 Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....\n",
      "Score: 0.78, Text: Mohsan et al. , “V ision Transformer and Language Model Based\n",
      "Radiology Report Generation, ” IEEE Access , vol. 11, pp. 1814-1824,\n",
      "2023. 8, 9, 10, 16, 18\n",
      "[96] Xian Wu et al. . “DeltaNet: Conditional M...\n",
      "Score: 0.77, Text: on datasets and metrics of textual question answering,” arXiv preprint\n",
      "arXiv:2109.12264, 2021. [203] H.-Y . Huang, E....\n",
      "Score: 0.77, Text: References\n",
      "[1] Omar Alfarghaly, Rana Khaled, Abeer Elkorany, Maha\n",
      "Helal, and Aly Fahmy. Automated radiology report genera-\n",
      "tion using conditioned transformers. IMU, 24:100557, 2021. 2, 4\n",
      "[2] Satanjeev...\n",
      "Score: 0.76, Text: 10, 11, 12, 16, 18\n",
      "[126] H. W ang et al., “Embracing Uniqueness: Generating Radiology Reports\n",
      "via a Transformer with Graph-based Distinctive Attention, ” IEEE Int. Conf. on Bioinform. and Biomed., pp....\n",
      "Score: 0.76, Text: AUTHOR et al. : PREP ARA TION OF P APERS FOR IEEE TRANSACTIONS AND JOURNALS ( FEBRUARY 2023) 23\n",
      "[84] J. Delbrouck et al. , “Improving the Factual Correctness of Radiology\n",
      "Report Generation with Semant...\n",
      "Score: 0.76, Text: Med. Unlocked , V ol. 24, No. 100557,\n",
      "2021. 10, 12, 16, 17, 18, 20, 21\n",
      "[121] G. Liu et al. , “Medical-VLBER T: Medical V isual Language BER T\n",
      "for COVID-19 CT Report Generation with Alternate Learning....\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_query = \"How Radiology Report Generation is achieved using transfer learning?\"\n",
    "\n",
    "start = time.time()\n",
    "matches, _ = retrieve(test_query)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Retrieval time: {elapsed:.4f}s\")\n",
    "for m in matches:\n",
    "    print(f\"Score: {m['score']:.2f}, Text: {m['metadata']['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb77f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def mmr(query_emb, doc_embs, texts, top_k=5, lambda_param=0.7):\n",
    "    doc_embs = np.array(doc_embs)\n",
    "    query_emb = np.array(query_emb).reshape(1, -1)\n",
    "    sim = cosine_similarity(doc_embs, query_emb).flatten()\n",
    "    selected, candidates = [], list(range(len(texts)))\n",
    "    while len(selected) < top_k and candidates:\n",
    "        if not selected:\n",
    "            idx = int(np.argmax(sim))\n",
    "        else:\n",
    "            sim_to_query = sim[candidates]\n",
    "            sim_to_selected = np.max(cosine_similarity(doc_embs[candidates], doc_embs[selected]), axis=1)\n",
    "            mmr_scores = lambda_param * sim_to_query - (1 - lambda_param) * sim_to_selected\n",
    "            idx = candidates[int(np.argmax(mmr_scores))]\n",
    "        selected.append(idx)\n",
    "        candidates.remove(idx)\n",
    "    return [texts[i] for i in selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "606562ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top reranked doc: 21, 2022. 2, 10, 14,\n",
      "16, 18, 19\n",
      "[31] Z. W ang et al. , “ A Medical Semantic-Assisted Transformer for Radio-\n",
      "graphic Report Generation. ”, Med. Image Comput. Assist Interv ., V ol\n",
      "13433, pp. 655–664 , 2022. 2, 8, 10, 12, 13, 14, 16, 19\n",
      "[32] H. Nguyen et al., “ Automated Generation of Accurate & Fluen\n"
     ]
    }
   ],
   "source": [
    "# 1. Get matches and query embedding\n",
    "matches, query_emb = retrieve(\"How Radiology Report Generation is achieved using transfer learning?\", top_k=10)\n",
    "\n",
    "# 2. Prepare for MMR\n",
    "doc_embs = [m['values'] for m in matches]\n",
    "texts = [m['metadata']['text'] for m in matches]\n",
    "\n",
    "# 3. Rerank\n",
    "reranked_mmr = mmr(query_emb, doc_embs, texts, top_k=5)\n",
    "print(\"Top reranked doc:\", reranked_mmr[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d21ad881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output:\n",
      " content=\"The provided text focuses on papers using transformer models and doesn't detail how transfer learning is specifically applied to radiology report generation.  More information is needed to answer the question.\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--4405bbdd-d9cf-4fa8-a4da-748a7afbf41a-0' usage_metadata={'input_tokens': 364, 'output_tokens': 36, 'total_tokens': 400, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "prompt_template = \"\"\"You are a research assistant. Use the context below to answer the question concisely.\n",
    "Context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "context = \"\\n\\n\".join(reranked_mmr[:3])\n",
    "question = test_query\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "final_prompt = prompt.format(context=context, question=question)\n",
    "llm_response = llm.invoke(final_prompt)\n",
    "print(\"LLM output:\\n\", llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e7735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "790326be",
   "metadata": {},
   "source": [
    "# LangChain Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edf295bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.7}  # Hyperparameter: tune as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d3d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question} \n",
    "        Context: {context} \n",
    "        Answer:\"\"\",\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6882a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3. DOC FORMATTER ----\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca215a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4. RAG CHAIN ----\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da589e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5. INTERACTIVE QUERY FUNCTION ----\n",
    "\n",
    "def run_rag_pipeline(question):\n",
    "    print(f\"\\n=== User Query ===\\n{question}\\n\")\n",
    "\n",
    "    # Retrieve context documents (for transparency and debug)\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} context documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        preview = doc.page_content[:300] + (\"...\" if len(doc.page_content) > 200 else \"\")\n",
    "        print(f\"{i}. {preview}  [source: {doc.metadata.get('source','')}]\")\n",
    "\n",
    "    print(\"\\nGenerating LLM Answer...\\n\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(\"=== LLM Answer ===\\n\", answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35685a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== User Query ===\n",
      "How Radiology Report Generation is achieved using transfer learning?\n",
      "\n",
      "Retrieved 4 context documents:\n",
      "1. 21, 2022. 2, 10, 14,\n",
      "16, 18, 19\n",
      "[31] Z. W ang et al. , “ A Medical Semantic-Assisted Transformer for Radio-\n",
      "graphic Report Generation. ”, Med. Image Comput. Assist Interv ., V ol\n",
      "13433, pp. 655–664 , 2022. 2, 8, 10, 12, 13, 14, 16, 19\n",
      "[32] H. Nguyen et al., “ Automated Generation of Accurate & Fluen...  [source: ]\n",
      "2. 22 IEEE REVIEWS IN BIOMEDICAL ENGINEERING, VOL. XX, NO. XX, X XXX 2023\n",
      "[33] C. Shang et al., “MA TNet: Exploiting Multi-Modal Features for Radiol-\n",
      "ogy Report Generation, ” IEEE Signal Process. Lett. , vol. 29, pp. 2692–\n",
      "2696, 2022. 2, 8, 9, 10, 13, 14, 15, 16, 18, 19\n",
      "[34] F . Dalla Serra et al., “Mu...  [source: ]\n",
      "3. Informatics in Medicine Unlocked 24 (2021) 100557\n",
      "Available online 26 March 2021\n",
      "2352-9148/© 2021 The Authors. Published by Elsevier Ltd.  [source: ]\n",
      "4. . 8\n",
      "2.2 Training Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  [source: ]\n",
      "\n",
      "Generating LLM Answer...\n",
      "\n",
      "=== LLM Answer ===\n",
      " Transfer learning in radiology report generation uses pre-trained models (e.g., transformers)  trained on large datasets of text and images.  These models are then fine-tuned on a radiology-specific dataset to generate reports.  This approach leverages the knowledge learned from the general data to improve performance on the smaller, specialized dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Transfer learning in radiology report generation uses pre-trained models (e.g., transformers)  trained on large datasets of text and images.  These models are then fine-tuned on a radiology-specific dataset to generate reports.  This approach leverages the knowledge learned from the general data to improve performance on the smaller, specialized dataset.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_rag_pipeline(\"How Radiology Report Generation is achieved using transfer learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167c741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b0a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1436d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c5a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037642c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03e18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69be694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
