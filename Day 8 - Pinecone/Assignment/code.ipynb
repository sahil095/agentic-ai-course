{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a736bded",
   "metadata": {},
   "source": [
    "# Pinecone Retrieval Pipeline with Reranking for RAG\n",
    "\n",
    "### Modular Pinecone Retrieval Pipeline for RAG\n",
    "\n",
    "This notebook demonstrates an end-to-end retrieval pipeline for a Retrieval-Augmented Generation (RAG) system using Pinecone as the vector database and GoogleGenerativeAIEmbeddings for embedding creation.\n",
    "\n",
    "The pipeline supports modular retrieval, optional Maximal Marginal Relevance (MMR) reranking, and is easily extendable for additional features such as BM25 reranking, LLM-based generation, or custom filtering.\n",
    "\n",
    "**Contents:**\n",
    "- Setup and Prerequisites\n",
    "- Retriever Function\n",
    "- MMR Reranking Function\n",
    "- Pipeline Wrapper\n",
    "- Usage Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775646b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c241401",
   "metadata": {},
   "source": [
    "## Set up your Environment and API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82f2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ['PINECONE_API_KEY']=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aada6c",
   "metadata": {},
   "source": [
    "## **Data Loading and Semantic Chunking Pipeline**\n",
    "\n",
    "This code block performs two key preprocessing steps to prepare your PDFs for retrieval-augmented generation (RAG):\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Load All PDFs from a Directory**\n",
    "\n",
    "- **Reads all PDF files** from the `./data/` folder using a file pattern (`*.pdf`).\n",
    "- **Loads each PDF** using `PyPDFLoader`, which extracts the content (typically one Document per page).\n",
    "- **Aggregates all page Documents** from all PDFs into a single list, `all_docs`.\n",
    "- **Prints the total number of loaded page documents**—helpful for verifying that your data covers the expected number of pages (should be >200 for a large-scale use case).\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Semantic Chunking of the Documents**\n",
    "\n",
    "- **Initializes your embedding model** (`GoogleGenerativeAIEmbeddings`), which will be used to embed text and assist semantic chunking.\n",
    "- **Creates a `SemanticChunker`** with:\n",
    "    - The embedding model\n",
    "    - A breakpoint method (`standard_deviation`) for smart, context-aware chunking (not just fixed-length)\n",
    "    - `min_chunk_size` to ensure chunks are not too small.\n",
    "- **Uses the chunker to split each document’s text** into semantically meaningful chunks, resulting in `semantic_chunks`.\n",
    "    - Each chunk is likely to contain a paragraph or section, preserving context for better retrieval and LLM answers.\n",
    "- **Prints the total number of semantic chunks** generated for tracking and diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Embed All Semantic Chunks**\n",
    "\n",
    "- **Extracts the raw text from each semantic chunk** into the list `texts`.\n",
    "- **Embeds each chunk** using the embedding model’s `.embed_documents()` method, resulting in a list of embedding vectors, one per chunk.\n",
    "- **Prints the length of one embedding vector** to confirm the embedding size (e.g., 768 for Google embeddings).\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "This code loads all your PDFs, splits them into high-quality semantic chunks using a neural chunker, and generates dense vector embeddings for each chunk. These embeddings and chunks are now ready to be added to your vector database for fast semantic search and retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64732170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load all PDFs from directory\n",
    "pdf_dir = \"./data/\"\n",
    "pdf_files = glob.glob(os.path.join(pdf_dir, \"*.pdf\"))\n",
    "\n",
    "all_docs = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    all_docs.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(all_docs)} page documents (should cover >200 pages).\")\n",
    "\n",
    "# Step 2: Semantic Chunking\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "chunker = SemanticChunker(embedding_model, breakpoint_threshold_type=\"standard_deviation\", min_chunk_size=100)\n",
    "semantic_chunks = chunker.create_documents([doc.page_content for doc in all_docs])\n",
    "\n",
    "print(f\"Total semantic chunks: {len(semantic_chunks)}\")\n",
    "\n",
    "texts = [chunk.page_content for chunk in semantic_chunks]\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484e30f",
   "metadata": {},
   "source": [
    "## **Index Creation and Data Ingestion with Pinecone**\n",
    "\n",
    "This code block sets up the Pinecone vector database, creates an index for storing your embeddings, and upserts (uploads) your embedded document chunks into the index for future retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Initialize Pinecone Client**\n",
    "\n",
    "- Instantiates a Pinecone client object, which provides methods to interact with the Pinecone service.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Create (or Connect to) the Vector Index**\n",
    "\n",
    "- **Defines the index name** (e.g., `\"agenticbatch2-assignment\"`).\n",
    "- **Checks if the index already exists**; if not, creates it with:\n",
    "    - `dimension=768`: The size of your embedding vectors (must match your model).\n",
    "    - `metric=\"cosine\"`: Uses cosine similarity for measuring vector similarity.\n",
    "    - `spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")`: Specifies the cloud provider and region for your index.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Load the Index and Initialize the Vector Store**\n",
    "\n",
    "- Loads the index for further operations.\n",
    "- Initializes a `PineconeVectorStore` using the loaded index and your embedding model.  \n",
    "  This makes it easy to add documents using high-level LangChain methods if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Prepare and Upsert Embedding Data**\n",
    "\n",
    "- **Creates a list of items** to insert:\n",
    "    - Each item is a tuple: `(unique_id, embedding_vector, metadata)`, where\n",
    "        - `unique_id`: A new UUID for each chunk,\n",
    "        - `embedding_vector`: The vector for the chunk,\n",
    "        - `metadata`: Typically stores the original text or provenance info.\n",
    "- **Uploads the items to Pinecone** in batches (batch size = 10), which is more efficient and avoids timeouts for large datasets.\n",
    "- **Upsert** is used so you can add new data or update existing data seamlessly.\n",
    "\n",
    "---\n",
    "\n",
    "**Result:**  \n",
    "After this block runs, your semantic document chunks are stored in the Pinecone index as vectors with metadata. You can now perform fast, scalable vecto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc=Pinecone()\n",
    "\n",
    "index_name=\"agenticbatch2-assignment\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")    \n",
    ")\n",
    "    \n",
    "#loading the index\n",
    "index=pc.Index(index_name)\n",
    "vector_store=PineconeVectorStore(index=index,embedding=embedding_model)\n",
    "\n",
    "# Each item: (id, vector, metadata)\n",
    "items = [\n",
    "    (str(uuid4()), emb, {\"text\": text}) for emb, text in zip(embeddings, texts)\n",
    "]\n",
    "\n",
    "batch_size = 10\n",
    "for i in range(0, len(items), batch_size):\n",
    "    index.upsert(vectors=items[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544865d6",
   "metadata": {},
   "source": [
    "## Optional: If you want to use different inexing techniques, you can create multiple indexes in Pinecode and test their performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc054e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDEX_DIM = 768  # Must match your embedding size\n",
    "\n",
    "# # Flat index\n",
    "# pc.create_index(\n",
    "#     name=\"rag-flat\",\n",
    "#     dimension=INDEX_DIM,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"gcp\", region=\"us-east1\"),\n",
    "#     pod_type=\"s1\",     # or appropriate for your plan\n",
    "#     index_type=\"POD\",  # POD is Pinecone's Flat index\n",
    "# )\n",
    "\n",
    "# # HNSW index\n",
    "# pc.create_index(\n",
    "#     name=\"rag-hnsw\",\n",
    "#     dimension=INDEX_DIM,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"gcp\", region=\"us-east1\"),\n",
    "#     pod_type=\"s1\",\n",
    "#     index_type=\"HNSW\"\n",
    "# )\n",
    "\n",
    "# # IVF_PQ index (Pinecone's version of IVF)\n",
    "# pc.create_index(\n",
    "#     name=\"rag-ivfpq\",\n",
    "#     dimension=INDEX_DIM,\n",
    "#     metric=\"cosine\",\n",
    "#     spec=ServerlessSpec(cloud=\"gcp\", region=\"us-east1\"),\n",
    "#     pod_type=\"s1\",\n",
    "#     index_type=\"IVF_PQ\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27bb29",
   "metadata": {},
   "source": [
    "## **Retriever Function for Semantic Search**\n",
    "\n",
    "This function enables semantic search over your Pinecone index using your embedding model. It takes a user query and retrieves the most relevant document chunks based on vector similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "- **Input:**  \n",
    "  - `query`: The user’s search string.\n",
    "  - `top_k`: The number of top results to retrieve (default: 10).\n",
    "  - `threshold`: Minimum similarity score for a match to be considered relevant.\n",
    "\n",
    "- **Process:**\n",
    "  1. **Embed the query:**  \n",
    "     The user’s query is converted into a dense vector using the embedding model (`embed_query`).\n",
    "  2. **Query the Pinecone index:**  \n",
    "     The query vector is compared against all vectors in the index to find the `top_k` most similar ones.\n",
    "     - `include_metadata=True`: Returns each match’s associated metadata (such as the original text).\n",
    "     - `include_values=True`: Returns each match’s embedding vector (useful for reranking).\n",
    "     - `filter=None`: No additional filtering is applied, but you can use this to restrict search by metadata (e.g., source or document type).\n",
    "  3. **Score threshold filtering:**  \n",
    "     Only results with a similarity `score` greater than or equal to `threshold` are kept.\n",
    "  4. **Returns:**  \n",
    "     - `matches`: The filtered list of top matches (each including metadata and embedding).\n",
    "     - `query_emb`: The embedding vector for the user’s query (useful for reranking or analysis).\n",
    "\n",
    "---\n",
    "\n",
    "**Usage:**  \n",
    "Call this function with a query string to get the most semantically similar chunks from your vector database for downstream processing, reranking, or as LLM context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=10, threshold=0.7):\n",
    "    query_emb = embedding_model.embed_query(query)\n",
    "    # Query Pinecone\n",
    "    result = index.query(\n",
    "        vector=query_emb,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter=None,  # Optionally filter on metadata\n",
    "        include_values=True\n",
    "    )\n",
    "    # Filter by score threshold\n",
    "    matches = [m for m in result['matches'] if m['score'] >= threshold]\n",
    "    return matches, query_emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235fccc",
   "metadata": {},
   "source": [
    "## **Benchmarking Retrieval Speed and Inspecting Results**\n",
    "\n",
    "This code block demonstrates how to **measure the response time** of your retrieval pipeline and to **inspect the top retrieved results** for a specific query.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "- **Defines a sample query** (`test_query`) relevant to your domain.\n",
    "- **Records the current time** before running the retrieval function.\n",
    "- **Calls the `retrieve` function** with the test query to obtain relevant document matches from Pinecone.\n",
    "- **Calculates the retrieval time** by subtracting the start time from the current time after retrieval completes.\n",
    "- **Prints the retrieval latency** (in seconds), giving you a sense of system performance.\n",
    "- **Iterates over the returned matches** to print:\n",
    "    - The similarity score of each result (how well it matches the query).\n",
    "    - The first 200 characters of the retrieved text for a quick preview.\n",
    "\n",
    "---\n",
    "\n",
    "**Usage:**  \n",
    "This benchmarking step is useful for evaluating the speed and quality of your search pipeline, tuning hyperparameters (like `top_k` or `threshold`), or comparing different index types (Flat, HNSW, IVF_PQ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c58b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "test_query = \"How Radiology Report Generation is achieved using transfer learning?\"\n",
    "\n",
    "start = time.time()\n",
    "matches, _ = retrieve(test_query)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Retrieval time: {elapsed:.4f}s\")\n",
    "for m in matches:\n",
    "    print(f\"Score: {m['score']:.2f}, Text: {m['metadata']['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e52921",
   "metadata": {},
   "source": [
    "## **Reranking Search Results with Maximal Marginal Relevance (MMR)**\n",
    "\n",
    "This code block introduces **MMR (Maximal Marginal Relevance) reranking** to further improve the quality of retrieved documents.  \n",
    "MMR helps ensure the final selected context is both highly relevant to the query and non-redundant (diverse), reducing repetition and surfacing varied supporting evidence.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "1. **MMR Function Definition**\n",
    "    - `mmr()` takes the query embedding, document embeddings, texts, and reranking parameters.\n",
    "    - It calculates the cosine similarity between the query and each document.\n",
    "    - It then iteratively selects the next document that maximizes both relevance to the query and diversity from already selected documents.\n",
    "    - `lambda_param` (typically between 0.5 and 0.8) controls the trade-off between relevance (higher) and diversity (lower).\n",
    "\n",
    "2. **Applying the MMR Pipeline**\n",
    "    - Calls `retrieve()` with a user query to get candidate matches (including their embeddings and texts).\n",
    "    - Extracts all document embeddings and corresponding texts from the matches.\n",
    "    - Passes these to `mmr()` to select the top `k` (e.g., 5) reranked chunks.\n",
    "    - Prints the top reranked document for inspection.\n",
    "\n",
    "---\n",
    "\n",
    "**Usage:**  \n",
    "MMR reranking is a best practice in RAG systems to reduce answer redundancy, increase answer quality, and help LLMs synthesize responses from a broader context set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb77f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def mmr(query_emb, doc_embs, texts, top_k=5, lambda_param=0.7):\n",
    "    doc_embs = np.array(doc_embs)\n",
    "    query_emb = np.array(query_emb).reshape(1, -1)\n",
    "    sim = cosine_similarity(doc_embs, query_emb).flatten()\n",
    "    selected, candidates = [], list(range(len(texts)))\n",
    "    while len(selected) < top_k and candidates:\n",
    "        if not selected:\n",
    "            idx = int(np.argmax(sim))\n",
    "        else:\n",
    "            sim_to_query = sim[candidates]\n",
    "            sim_to_selected = np.max(cosine_similarity(doc_embs[candidates], doc_embs[selected]), axis=1)\n",
    "            mmr_scores = lambda_param * sim_to_query - (1 - lambda_param) * sim_to_selected\n",
    "            idx = candidates[int(np.argmax(mmr_scores))]\n",
    "        selected.append(idx)\n",
    "        candidates.remove(idx)\n",
    "    return [texts[i] for i in selected]\n",
    "\n",
    "\n",
    "# 1. Get matches and query embedding\n",
    "matches, query_emb = retrieve(\"How Radiology Report Generation is achieved using transfer learning?\", top_k=10)\n",
    "\n",
    "# 2. Prepare for MMR\n",
    "doc_embs = [m['values'] for m in matches]\n",
    "texts = [m['metadata']['text'] for m in matches]\n",
    "\n",
    "# 3. Rerank\n",
    "reranked_mmr = mmr(query_emb, doc_embs, texts, top_k=5)\n",
    "print(\"Top reranked doc:\", reranked_mmr[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cadb99",
   "metadata": {},
   "source": [
    "## **LLM Answer Generation with Prompt Engineering**\n",
    "\n",
    "This code block takes the final, reranked retrieved context and uses it as input to a Large Language Model (LLM) to generate a concise, context-grounded answer.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "\n",
    "1. **Prompt Template Construction**\n",
    "    - Defines a clear, instruction-based prompt for the LLM, specifying its role as a research assistant.\n",
    "    - Uses placeholders `{context}` and `{question}` to dynamically insert the most relevant document snippets and the user's question.\n",
    "\n",
    "2. **Prepare Context and Question**\n",
    "    - Selects the top 3 reranked document chunks (`reranked_mmr[:3]`), joining them into a single string as context.\n",
    "    - Uses the test query as the question.\n",
    "\n",
    "3. **LLM Initialization and Invocation**\n",
    "    - Initializes the LLM (Google Gemini) using `ChatGoogleGenerativeAI`.\n",
    "    - Fills the prompt template with the selected context and question.\n",
    "    - Sends the filled prompt to the LLM for answer generation.\n",
    "    - Prints the generated answer.\n",
    "\n",
    "---\n",
    "\n",
    "**Usage:**  \n",
    "This step combines retrieved, high-quality context with the user’s question, then leverages the LLM’s reasoning and synthesis abilities to produce a focused, well-grounded answer—closing the loop of the RAG pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ad881",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a research assistant. Use the context below to answer the question concisely.\n",
    "Context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "context = \"\\n\\n\".join(reranked_mmr[:3])\n",
    "question = test_query\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "final_prompt = prompt.format(context=context, question=question)\n",
    "llm_response = llm.invoke(final_prompt)\n",
    "print(\"LLM output:\\n\", llm_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790326be",
   "metadata": {},
   "source": [
    "# LangChain Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf295bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"score_threshold\": 0.7}  # Hyperparameter: tune as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question} \n",
    "        Context: {context} \n",
    "        Answer:\"\"\",\n",
    "    input_variables=['context', 'question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6882a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3. DOC FORMATTER ----\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca215a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4. RAG CHAIN ----\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da589e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5. INTERACTIVE QUERY FUNCTION ----\n",
    "\n",
    "def run_rag_pipeline(question):\n",
    "    print(f\"\\n=== User Query ===\\n{question}\\n\")\n",
    "\n",
    "    # Retrieve context documents (for transparency and debug)\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} context documents:\")\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        preview = doc.page_content[:300] + (\"...\" if len(doc.page_content) > 200 else \"\")\n",
    "        print(f\"{i}. {preview}  [source: {doc.metadata.get('source','')}]\")\n",
    "\n",
    "    print(\"\\nGenerating LLM Answer...\\n\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(\"=== LLM Answer ===\\n\", answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35685a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rag_pipeline(\"How Radiology Report Generation is achieved using transfer learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167c741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b0a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1436d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5c5a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037642c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03e18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69be694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
