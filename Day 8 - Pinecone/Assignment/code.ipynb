{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6131d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Krish Naik Youtube\\Agentic AI Course\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from uuid import uuid4\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82f2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_TOKEN']=os.getenv(\"HF_TOKEN\")\n",
    "os.environ['GOOGLE_API_KEY']=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ['PINECONE_API_KEY']=os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64732170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load all PDFs from directory\n",
    "pdf_dir = \"./data/\"\n",
    "pdf_files = glob.glob(os.path.join(pdf_dir, \"*.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa1f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []\n",
    "for pdf_file in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    all_docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c046653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 247 page documents (should cover >200 pages).\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(all_docs)} page documents (should cover >200 pages).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c678d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total semantic chunks: 308\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Semantic Chunking\n",
    "embedder = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
    "chunker = SemanticChunker(embedder, breakpoint_threshold_type=\"standard_deviation\", min_chunk_size=200)\n",
    "semantic_chunks = chunker.create_documents([doc.page_content for doc in all_docs])\n",
    "\n",
    "print(f\"Total semantic chunks: {len(semantic_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d3fb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Journal of Machine Learning Research 21 (2020) 1-67 Submitted 1/20; Revised 6/20; Published 6/20\\nExploring the Limits of Transfer Learning with a Unified\\nText-to-Text Transformer\\nColin Raffel∗ craffel@gmail.com\\nNoam Shazeer∗ noam@google.com\\nAdam Roberts∗ adarob@google.com\\nKatherine Lee∗ katherinelee@google.com\\nSharan Narang sharannarang@google.com\\nMichael Matena mmatena@google.com\\nYanqi Zhou yanqiz@google.com\\nWei Li mweili@google.com\\nPeter J. Liu peterjliu@google.com\\nGoogle, Mountain View, CA 94043, USA\\nEditor: Ivan Titov\\nAbstract\\nTransfer learning, where a model is first pre-trained on a data-rich task before being fine-\\ntuned on a downstream task, has emerged as a powerful technique in natural language\\nprocessing (NLP). The effectiveness of transfer learning has given rise to a diversity of\\napproaches, methodology, and practice. In this paper, we explore the landscape of transfer\\nlearning techniques for NLP by introducing a unified framework that converts all text-based\\nlanguage problems into a text-to-text format. Our systematic study compares pre-training\\nobjectives, architectures, unlabeled data sets, transfer approaches, and other factors on\\ndozens of language understanding tasks. By combining the insights from our exploration\\nwith scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results\\non many benchmarks covering summarization, question answering, text classification, and\\nmore. To facilitate future work on transfer learning for NLP, we release our data set,\\npre-trained models, and code.1\\nKeywords: transfer learning, natural language processing, multi-task learning, attention-\\nbased models, deep learning\\n1. Introduction\\nTraining a machine learning model to perform natural language processing (NLP) tasks\\noften requires that the model can process text in a way that is amenable to downstream\\nlearning. This can be loosely viewed as developing general-purpose knowledge that allows\\nthe model to “understand” text. This knowledge can range from low-level (e.g. the spelling\\n∗. Equal contribution. A description of each author’s contribution is available in Appendix A. Correspondence\\nto craffel@gmail.com. 1. https://github.com/google-research/text-to-text-transfer-transformer\\n©2020 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\\nLi, and Peter J. Liu. License: CC-BY 4.0, seehttps://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at\\nhttp://jmlr.org/papers/v21/20-074.html. arXiv:1910.10683v4  [cs.LG]  19 Sep 2023'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_chunks[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4174c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [chunk.page_content for chunk in semantic_chunks]\n",
    "embeddings = embedder.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c8762a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e28f4484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc=Pinecone()\n",
    "\n",
    "index_name=\"agenticbatch2-assignment\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=768,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "521879f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the index\n",
    "index=pc.Index(index_name)\n",
    "vector_store=PineconeVectorStore(index=index,embedding=embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "779ff493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each item: (id, vector, metadata)\n",
    "items = [\n",
    "    (str(uuid4()), emb, {\"text\": text}) for emb, text in zip(embeddings, texts)\n",
    "]\n",
    "# uuids = [str(uuid4()) for _ in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0c18989",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "for i in range(0, len(items), batch_size):\n",
    "    index.upsert(vectors=items[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f51571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=5, threshold=0.7):\n",
    "    query_emb = embedder.embed_query(query)\n",
    "    # Query Pinecone\n",
    "    result = index.query(\n",
    "        vector=query_emb,\n",
    "        top_k=top_k,\n",
    "        include_metadata=True,\n",
    "        filter=None,  # Optionally filter on metadata\n",
    "        include_values=True\n",
    "    )\n",
    "    # Filter by score threshold\n",
    "    matches = [m for m in result['matches'] if m['score'] >= threshold]\n",
    "    return matches, query_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47c58b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval time: 1.0249s\n",
      "Score: 0.72, Text: V. P OPULAR DATASETS FOR LLM S\n",
      "Large language models exhibit promising accomplish-\n",
      "ments, but the main question that arises is how effectively\n",
      "they function and how their performance can be assessed i...\n",
      "Score: 0.72, Text: 7. A figure showing the training time of the custom recurrent model \n",
      "VSGRU and the transformer-based model CDGPT2 . O. Alfarghaly et al....\n",
      "Score: 0.70, Text: Contents\n",
      "1 Introduction 3\n",
      "1.1 Key Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
      "1.2 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . ....\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "test_query = \"What are the main findings about transformer models?\"\n",
    "\n",
    "start = time.time()\n",
    "matches, _ = retrieve(test_query)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Retrieval time: {elapsed:.4f}s\")\n",
    "for m in matches:\n",
    "    print(f\"Score: {m['score']:.2f}, Text: {m['metadata']['text'][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb77f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reranking\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def mmr(query_emb, doc_embs, texts, top_k=5, lambda_param=0.7):\n",
    "    doc_embs = np.array(doc_embs)\n",
    "    query_emb = np.array(query_emb).reshape(1, -1)\n",
    "    sim = cosine_similarity(doc_embs, query_emb).flatten()\n",
    "    selected, candidates = [], list(range(len(texts)))\n",
    "    while len(selected) < top_k and candidates:\n",
    "        if not selected:\n",
    "            idx = int(np.argmax(sim))\n",
    "        else:\n",
    "            sim_to_query = sim[candidates]\n",
    "            sim_to_selected = np.max(cosine_similarity(doc_embs[candidates], doc_embs[selected]), axis=1)\n",
    "            mmr_scores = lambda_param * sim_to_query - (1 - lambda_param) * sim_to_selected\n",
    "            idx = candidates[int(np.argmax(mmr_scores))]\n",
    "        selected.append(idx)\n",
    "        candidates.remove(idx)\n",
    "    return [texts[i] for i in selected]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "606562ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top reranked doc: V. P OPULAR DATASETS FOR LLM S\n",
      "Large language models exhibit promising accomplish-\n",
      "ments, but the main question that arises is how effectively\n",
      "they function and how their performance can be assessed in\n",
      "specific tasks or applications.\n"
     ]
    }
   ],
   "source": [
    "# 1. Get matches and query embedding\n",
    "matches, query_emb = retrieve(\"What are the main findings about transformer models?\", top_k=10)\n",
    "\n",
    "# 2. Prepare for MMR\n",
    "doc_embs = [m['values'] for m in matches]\n",
    "texts = [m['metadata']['text'] for m in matches]\n",
    "\n",
    "# 3. Rerank\n",
    "reranked_mmr = mmr(query_emb, doc_embs, texts, top_k=5)\n",
    "print(\"Top reranked doc:\", reranked_mmr[0][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d21ad881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output:\n",
      " content='The provided text does not offer findings about transformer models beyond mentioning CDGPT2 as a transformer-based model whose training time is compared to another model (VSGRU).  No specific performance results or conclusions about transformer models are given.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []} id='run--9e59c749-047a-476f-bf65-e203b52620ff-0' usage_metadata={'input_tokens': 229, 'output_tokens': 47, 'total_tokens': 276, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "prompt_template = \"\"\"You are a research assistant. Use the context below to answer the question concisely.\n",
    "Context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "context = \"\\n\\n\".join(reranked_mmr[:3])\n",
    "question = test_query\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "final_prompt = prompt.format(context=context, question=question)\n",
    "llm_response = llm.invoke(final_prompt)\n",
    "print(\"LLM output:\\n\", llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ac0e7735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output to rag_llm_output.docx\n"
     ]
    }
   ],
   "source": [
    "from docx import Document as DocxDoc\n",
    "\n",
    "docx = DocxDoc()\n",
    "docx.add_heading('RAG Output', 0)\n",
    "docx.add_heading('Question', level=1)\n",
    "docx.add_paragraph(question)\n",
    "docx.add_heading('Answer', level=1)\n",
    "docx.add_paragraph(str(llm_response))\n",
    "\n",
    "docx.save(\"rag_llm_output.docx\")\n",
    "print(\"Saved output to rag_llm_output.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790326be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf295bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3d687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
